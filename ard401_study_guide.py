import streamlit as st

st.set_page_config(
    page_title="ARD 401 - Mind Maps",
    page_icon="ğŸ§ ",
    layout="wide"
)

# Clean styling - dark background, white text, proper vertical formatting
st.markdown("""
    <style>
    body {
        background-color: #0f0f0f;
        color: #ffffff;
    }
    .stMarkdown {
        color: #ffffff;
    }
    h1, h2, h3, h4, h5, h6 {
        color: #ffffff !important;
    }
    .mind-map-container {
        background-color: #1a1a1a;
        border-left: 5px solid #667eea;
        padding: 25px;
        border-radius: 8px;
        margin: 20px 0;
        color: #ffffff;
        font-family: 'Courier New', monospace;
        font-size: 14px;
        line-height: 1.6;
        overflow-x: auto;
        white-space: pre-wrap;
        word-wrap: break-word;
    }
    .unit-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 20px;
        border-radius: 8px;
        margin-bottom: 20px;
        text-align: center;
    }
    </style>
""", unsafe_allow_html=True)

# Header
st.markdown("""
    <div class="unit-header">
        <h1 style="margin: 0; color: white;">ğŸ§  ARD 401 - Mind Maps</h1>
        <p style="margin: 10px 0; font-size: 1.1em; color: white;">Recommender Systems | Complete Vertical View</p>
    </div>
""", unsafe_allow_html=True)

# Create tabs
tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Œ Unit I", "ğŸ“ˆ Unit II", "ğŸŒ Unit III", "ğŸ›¡ï¸ Unit IV"])

# UNIT I
with tab1:
    st.markdown('<div class="unit-header"><h2>ğŸ“Œ UNIT I: Fundamentals & Collaborative Filtering</h2></div>', unsafe_allow_html=True)
    
    st.markdown("""
    <div class="mind-map-container">
UNIT I: FUNDAMENTALS & CF
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Œ RECOMMENDER SYSTEMS BASICS
â”œâ”€ Goals
â”‚  â”œâ”€ Personalization
â”‚  â”œâ”€ Discovery
â”‚  â”œâ”€ Engagement
â”‚  â””â”€ Retention & Revenue
â”œâ”€ Challenges
â”‚  â”œâ”€ Cold Start (new users/items)
â”‚  â”œâ”€ Sparsity (99.9% matrix empty)
â”‚  â”œâ”€ Scalability (O(mÂ²) complexity)
â”‚  â”œâ”€ Diversity (avoid boring recs)
â”‚  â””â”€ Bias (popularity, user bias)
â””â”€ Types
   â”œâ”€ Content-Based
   â”œâ”€ Collaborative Filtering
   â””â”€ Hybrid

ğŸ‘¥ USER-BASED COLLABORATIVE FILTERING
â”œâ”€ Concept: Similar users â†’ Similar preferences
â”œâ”€ Algorithm (Step-by-step):
â”‚  1. Calculate mean rating: rÌ„_u = Î£r_ui / n
â”‚  2. Find overlapping items (common rated items only)
â”‚  3. Compute Pearson correlation on overlapping
â”‚  4. Select k-nearest neighbors (k=10-20)
â”‚  5. Weighted average: rÌ‚_uj = rÌ„_u + Î£ sim(u,v)Ã—(r_vj - rÌ„_v) / Î£|sim|
â”œâ”€ Key Similarity Ranges:
â”‚  â”œâ”€ Pearson: -1 to +1
â”‚  â”œâ”€ Similar users: 0.7 to 1.0
â”‚  â”œâ”€ Moderate: 0.4 to 0.7
â”‚  â””â”€ Dissimilar: < 0.4
â””â”€ âš ï¸  CRITICAL: ALWAYS mean-center (r_u - rÌ„_u)

ğŸ“¦ ITEM-BASED COLLABORATIVE FILTERING
â”œâ”€ Concept: Similar items â†’ Rated similarly
â”œâ”€ Formula: rÌ‚_uj = Î£ sim(i,j)Ã—r_ui / Î£|sim|
â”œâ”€ âœ… Advantages:
â”‚  â”œâ”€ More stable than user-based
â”‚  â”œâ”€ Better for new users (need only 1 rating)
â”‚  â”œâ”€ Cacheable (compute offline)
â”‚  â””â”€ Predictable performance
â””â”€ âš ï¸  Note: Exclude negative similarities (-0.94 to 1.0)

âš¡ MATRIX FACTORIZATION (SVD)
â”œâ”€ Concept: R â‰ˆ U Ã— V^T
â”‚  â”œâ”€ m Ã— k user latent matrix
â”‚  â””â”€ n Ã— k item latent matrix
â”œâ”€ Prediction: rÌ‚_ij = u_i Â· v_j
â”œâ”€ Error Calculation: e_ij = r_ij - rÌ‚_ij
â”œâ”€ SGD Update (MOST IMPORTANT):
â”‚  â”œâ”€ u_i â† u_i + Î³(e_ij Ã— v_j - Î» Ã— u_i)
â”‚  â”œâ”€ v_j â† v_j + Î³(e_ij Ã— u_i - Î» Ã— v_j)
â”‚  â””â”€ â­ NEVER FORGET Î» term (prevents overfitting!)
â”œâ”€ Parameters:
â”‚  â”œâ”€ Î³ (learning rate): 0.001-0.1
â”‚  â”‚  â”œâ”€ Too high â†’ oscillates
â”‚  â”‚  â””â”€ Too low â†’ slow convergence
â”‚  â”œâ”€ Î» (regularization): 0.001-0.01
â”‚  â”‚  â””â”€ Controls overfitting on sparse data
â”‚  â”œâ”€ k (latent factors): 20-100
â”‚  â”‚  â””â”€ Number of hidden dimensions
â”‚  â””â”€ Convergence: 20-50 iterations
â””â”€ âš ï¸  CRITICAL: Always include Î»Ã—u_i regularization!

ğŸ¯ KEY CHALLENGES
â”œâ”€ â„ï¸  Cold Start
â”‚  â”œâ”€ Problem: New user/item â†’ No ratings exist
â”‚  â””â”€ Solution: Content-based, Hybrid, Knowledge-based
â”œâ”€ ğŸ“‰ Sparsity
â”‚  â”œâ”€ Problem: 99.9% of matrix is empty
â”‚  â””â”€ Solution: Dimensionality reduction, Clustering
â”œâ”€ âš¡ Scalability
â”‚  â”œâ”€ Problem: O(mÂ²) complexity (too slow)
â”‚  â””â”€ Solution: Item-based CF, Caching
â”œâ”€ âš–ï¸  Diversity
â”‚  â”œâ”€ Problem: High accuracy = boring recommendations
â”‚  â””â”€ Solution: Balance via regularization parameter Î»
â””â”€ ğŸ‘¥ Bias
   â”œâ”€ Problem: Popular items rated higher (natural bias)
   â””â”€ Solution: Debiasing techniques, Fairness metrics
    </div>
    """, unsafe_allow_html=True)

# UNIT II
with tab2:
    st.markdown('<div class="unit-header"><h2>ğŸ“ˆ UNIT II: Evaluation & Context-Aware Systems</h2></div>', unsafe_allow_html=True)
    
    st.markdown("""
    <div class="mind-map-container">
UNIT II: EVALUATION METRICS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š EVALUATION PARADIGMS
â”œâ”€ Offline Evaluation
â”‚  â”œâ”€ Method: Split data into 80% train, 20% test
â”‚  â”œâ”€ âœ… Pros: Fast, cheap, repeatable
â”‚  â””â”€ âŒ Cons: Metrics â‰  real user behavior
â”œâ”€ Online A/B Testing
â”‚  â”œâ”€ Method: Real users see algorithm A vs B
â”‚  â”œâ”€ âœ… Pros: Real behavior, business metrics
â”‚  â””â”€ âŒ Cons: Expensive, slow, risky
â””â”€ User Study
   â”œâ”€ Method: Recruit N=20-100 participants
   â”œâ”€ âœ… Pros: Capture subjective aspects (satisfaction)
   â””â”€ âŒ Cons: Small sample, low generalizability

ğŸ“Š RATING PREDICTION METRICS (Regression)
â”œâ”€ MAE (Mean Absolute Error)
â”‚  â”œâ”€ Formula: Î£|r - rÌ‚| / n
â”‚  â”œâ”€ Typical: 0.3-0.7 stars
â”‚  â””â”€ Easy to interpret (average error in stars)
â”œâ”€ RMSE (Root Mean Squared Error)
â”‚  â”œâ”€ Formula: âˆš[Î£(r - rÌ‚)Â² / n]
â”‚  â”œâ”€ Typical: 0.3-1.0 stars
â”‚  â””â”€ â­ MOST COMMONLY USED
â””â”€ MSE (Mean Squared Error)
   â”œâ”€ Formula: Î£(r - rÌ‚)Â² / n
   â””â”€ Same as RMSEÂ²

ğŸ“Š RANKING METRICS (Most Important!)
â”œâ”€ Precision@k
â”‚  â”œâ”€ Formula: (#relevant in top-k) / k
â”‚  â”œâ”€ Typical: 0.4-0.7
â”‚  â”œâ”€ Question: What % of recommendations are good?
â”‚  â””â”€ k=10 is common
â”œâ”€ Recall@k
â”‚  â”œâ”€ Formula: (#relevant in top-k) / (total relevant)
â”‚  â”œâ”€ Typical: 0.5-1.0
â”‚  â”œâ”€ Question: What % of user's items did we find?
â”‚  â””â”€ Higher k = Higher recall
â”œâ”€ NDCG@k â­ MOST SOPHISTICATED
â”‚  â”œâ”€ Full name: Normalized Discounted Cumulative Gain
â”‚  â”œâ”€ Formula: DCG / IDCG
â”‚  â”œâ”€ Why it matters: Position matters!
â”‚  â”‚  â”œâ”€ Item at position 1 = worth more
â”‚  â”‚  â”œâ”€ Item at position 10 = worth less
â”‚  â”‚  â””â”€ Log scale penalizes lower positions
â”‚  â”œâ”€ DCG Calculation:
â”‚  â”‚  â”œâ”€ DCG = Î£ [2^rel_i - 1] / logâ‚‚(i+1)
â”‚  â”‚  â”œâ”€ Relevant item at pos 1: (2Â¹-1) / logâ‚‚(2) = 1.0
â”‚  â”‚  â”œâ”€ Irrelevant item at pos 2: 0 / logâ‚‚(3) = 0
â”‚  â”‚  â”œâ”€ Relevant item at pos 3: (2Â¹-1) / logâ‚‚(4) = 0.5
â”‚  â”‚  â””â”€ Sum these up = DCG value
â”‚  â”œâ”€ IDCG: DCG if all items were ranked perfectly
â”‚  â”œâ”€ NDCG = DCG / IDCG (normalized between 0-1)
â”‚  â”œâ”€ Typical: 0.5-0.8
â”‚  â””â”€ âš ï¸  CRITICAL: Use logâ‚‚(i+1), NOT log(i)!
â””â”€ MAP (Mean Average Precision)
   â”œâ”€ Formula: Î£(Precision@k for each relevant) / |relevant|
   â”œâ”€ Typical: 0.4-0.8
   â””â”€ Captures precision at each relevant position

â° TEMPORAL COLLABORATIVE FILTERING
â”œâ”€ Why it matters: User preferences change over time
â”œâ”€ Exponential Decay Model:
â”‚  â”œâ”€ Formula: w(t) = e^{-Î»(t_current - t)}
â”‚  â”œâ”€ Î» = 0.01 (typical value)
â”‚  â”œâ”€ Half-life â‰ˆ 70 days
â”‚  â””â”€ Example: 1-day-old rating 2.3Ã— heavier than 95-day-old
â”œâ”€ Time-SVD++ Model:
â”‚  â”œâ”€ Formula: rÌ‚_uit = Î¼ + b_u(t) + b_i(t) + Î£_k u_uk Ã— v_ik(t)
â”‚  â”œâ”€ b_u(t) = user bias that changes over time
â”‚  â”œâ”€ b_i(t) = item bias that changes over time
â”‚  â””â”€ Captures both user drift AND item popularity trends
â””â”€ Key insight: Recent ratings matter more!

ğŸŒ CONTEXT-AWARE SYSTEMS
â”œâ”€ Multiple dimensions: Users Ã— Items Ã— Context
â”œâ”€ Example contexts:
â”‚  â”œâ”€ Location (home, work, traveling)
â”‚  â”œâ”€ Time (morning, evening, weekend)
â”‚  â”œâ”€ Device (phone, tablet, desktop)
â”‚  â”œâ”€ Weather (sunny, rainy, snowy)
â”‚  â””â”€ Social (alone, with friends, at party)
â”œâ”€ Multi-criteria example (Movie):
â”‚  â”œâ”€ Plot rating: 5 stars
â”‚  â”œâ”€ Music rating: 3 stars
â”‚  â”œâ”€ Effects rating: 4 stars
â”‚  â”œâ”€ Weights: [0.4, 0.3, 0.3]
â”‚  â””â”€ Overall: 0.4Ã—5 + 0.3Ã—3 + 0.3Ã—4 = 4.0
â””â”€ Approach: Factorize multi-dimensional tensor
    </div>
    """, unsafe_allow_html=True)

# UNIT III
with tab3:
    st.markdown('<div class="unit-header"><h2>ğŸŒ UNIT III: Structural Recommendations in Networks</h2></div>', unsafe_allow_html=True)
    
    st.markdown("""
    <div class="mind-map-container">
UNIT III: NETWORKS & LINK PREDICTION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ”— PAGERANK ALGORITHM â­ CORE
â”œâ”€ Concept: Important pages get links from important pages
â”œâ”€ Real-world: Google uses PageRank for search ranking
â”œâ”€ Formula: PR(p) = (1-d)/N + d Ã— Î£_{qâ†’p} [PR(q) / out(q)]
â”œâ”€ Parameters:
â”‚  â”œâ”€ d = damping factor = 0.85
â”‚  â”‚  â”œâ”€ Probability to follow a link = 85%
â”‚  â”‚  â””â”€ Probability to teleport = 15%
â”‚  â”œâ”€ (1-d)/N = teleport probability
â”‚  â”‚  â”œâ”€ With N=20 pages: (1-0.85)/20 â‰ˆ 0.0075
â”‚  â”‚  â””â”€ Each page gets equal 0.0075
â”‚  â”œâ”€ PR(q) = PageRank of page q linking to p
â”‚  â”‚  â””â”€ Vote from q depends on its own importance
â”‚  â””â”€ out(q) = number of outgoing links from q
â”‚     â””â”€ Divide PR equally among all outgoing links
â”œâ”€ Calculation Example (3 pages):
â”‚  â”œâ”€ Initial: PR(A)=PR(B)=PR(C)=1/3 â‰ˆ 0.333
â”‚  â”œâ”€ Iteration 1:
â”‚  â”‚  â”œâ”€ PR(A) = 0.05 + 0.85Ã—(...calculations...) = 0.05
â”‚  â”‚  â”œâ”€ PR(B) = 0.05 + 0.85Ã—(...calculations...) = 0.192
â”‚  â”‚  â””â”€ PR(C) = 0.05 + 0.85Ã—(...calculations...) = 0.475
â”‚  â”œâ”€ Iteration 2: Recalculate using new PR values
â”‚  â””â”€ Convergence: ~20 iterations, then stabilizes
â””â”€ âš ï¸  CRITICAL: Always normalize so Î£ PR = 1!
   â””â”€ Sum of all PageRanks must equal 1.0

ğŸ” LINK PREDICTION METRICS
â”œâ”€ Common Neighbors (CN)
â”‚  â”œâ”€ Formula: |N(A) âˆ© N(B)|
â”‚  â”œâ”€ Simplest approach
â”‚  â”œâ”€ Example: A and B have 2 mutual friends
â”‚  â””â”€ CN(A,B) = 2
â”œâ”€ Jaccard Index
â”‚  â”œâ”€ Formula: |N(A) âˆ© N(B)| / |N(A) âˆª N(B)|
â”‚  â”œâ”€ Normalized version of CN
â”‚  â”œâ”€ Range: 0 to 1
â”‚  â””â”€ Example: If A,B have 2 commons, 4 total = 2/4 = 0.5
â”œâ”€ Adamic-Adar â­ USUALLY BEST
â”‚  â”œâ”€ Formula: Î£_{w âˆˆ N(A)âˆ©N(B)} [1 / log|N(w)|]
â”‚  â”œâ”€ Key: Weight mutual friends by their degree
â”‚  â”‚  â”œâ”€ Friend with few friends â†’ higher weight
â”‚  â”‚  â””â”€ Friend with many friends â†’ lower weight
â”‚  â”œâ”€ Example:
â”‚  â”‚  â”œâ”€ Mutual friend C has 4 total friends
â”‚  â”‚  â”‚  â”œâ”€ Weight = 1/log(4) = 0.722
â”‚  â”‚  â”œâ”€ Mutual friend D has 3 total friends
â”‚  â”‚  â”‚  â””â”€ Weight = 1/log(3) = 0.910
â”‚  â”‚  â””â”€ AA(A,B) = 0.722 + 0.910 = 1.632
â”‚  â”œâ”€ Intuition: Rare common connections are more valuable
â”‚  â””â”€ Typical performance: Better than CN and Jaccard
â””â”€ Katz Index (Most sophisticated)
   â”œâ”€ Formula: Î£_â„“ Î²^â„“ Ã— (# paths of length â„“)
   â”œâ”€ Considers ALL paths between nodes
   â”œâ”€ Î² = damping factor (0 < Î² < 1)
   â”œâ”€ Path of length 1: Direct connection
   â”œâ”€ Path of length 2: Through 1 intermediate
   â”œâ”€ Path of length 3: Through 2 intermediates
   â””â”€ Longer paths get exponentially less weight

ğŸ‘¥ TRUST-CENTRIC RECOMMENDATION
â”œâ”€ Concept: Use explicit trust instead of implicit similarity
â”œâ”€ Formula: rÌ‚_uj = Î£_v [trust(u,v) Ã— r_vj] / Î£ trust
â”œâ”€ Why better than CF:
â”‚  â”œâ”€ âœ… Robust to attacks (attackers have NO trust)
â”‚  â”œâ”€ âœ… Better cold-start (explicit trust available)
â”‚  â”œâ”€ âœ… More transparent (users understand why)
â”‚  â””â”€ âœ… Explicit relationships (more reliable)
â”œâ”€ Trust Propagation:
â”‚  â”œâ”€ Direct trust: Aâ†’B only
â”‚  â”œâ”€ Transitive trust: Aâ†’Bâ†’C (diminished by distance)
â”‚  â”‚  â”œâ”€ trust(A,C) = trust(A,B) Ã— trust(B,C) Ã— decay
â”‚  â”‚  â””â”€ Decay = e^{-Î»Ã—distance}
â”‚  â”œâ”€ Weighted trust: Different trust levels per edge
â”‚  â””â”€ Filtered trust: Only high-trust edges matter
â””â”€ Real-world: Epinions, Slashdot use trust networks

ğŸ“Š HITS ALGORITHM
â”œâ”€ Full name: Hypertext Induced Topic Search
â”œâ”€ Two scores per node:
â”‚  â”œâ”€ Hub Score: How many authorities does it link to?
â”‚  â”‚  â””â”€ Good hub = links to many good authorities
â”‚  â””â”€ Authority Score: How many hubs link to it?
â”‚     â””â”€ Good authority = many good hubs link to it
â”œâ”€ Iterative algorithm:
â”‚  â”œâ”€ Step 1: Initialize all scores = 1/N
â”‚  â”œâ”€ Step 2: For each iteration:
â”‚  â”‚  â”œâ”€ authority(p) = Î£ hub(q) for all qâ†’p
â”‚  â”‚  â””â”€ hub(p) = Î£ authority(r) for all pâ†’r
â”‚  â”œâ”€ Step 3: Normalize both scores (Î£ = 1)
â”‚  â””â”€ Step 4: Repeat until convergence (~20 iterations)
â””â”€ vs PageRank: HITS is query-specific, PR is global

ğŸŒ SOCIAL INFLUENCE MODELS
â”œâ”€ Linear Threshold Model
â”‚  â”œâ”€ User adopts when influenced neighbors â‰¥ threshold
â”‚  â”œâ”€ Example: Buy item if 3+ friends bought it
â”‚  â””â”€ Deterministic (threshold-based)
â”œâ”€ Cascade Model
â”‚  â”œâ”€ Sequential adoption spread through network
â”‚  â”œâ”€ Example: User sees friend bought â†’ might buy
â”‚  â””â”€ Probabilistic (influenced neighbors)
â””â”€ Independent Cascade Model
   â”œâ”€ Each user makes independent decision
   â”œâ”€ Influenced by neighbors but randomized
   â””â”€ More realistic for real social networks
    </div>
    """, unsafe_allow_html=True)

# UNIT IV
with tab4:
    st.markdown('<div class="unit-header"><h2>ğŸ›¡ï¸ UNIT IV: Advanced Topics & Robustness</h2></div>', unsafe_allow_html=True)
    
    st.markdown("""
    <div class="mind-map-container">
UNIT IV: ADVANCED & ROBUSTNESS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸš¨ SHILLING ATTACK DETECTION âš ï¸  KEY METRIC: VARIANCE
â”œâ”€ What is shilling: Fake accounts rating to manipulate recommendations
â”œâ”€ Attacker vs Normal User Comparison:
â”‚  â”œâ”€ VARIANCE (Most important!):
â”‚  â”‚  â”œâ”€ Normal user: 0.3-0.5 (consistent preferences)
â”‚  â”‚  â””â”€ Attacker: 1.2-2.0 (random or biased) â­ 4Ã— HIGHER!
â”‚  â”œâ”€ Distribution pattern:
â”‚  â”‚  â”œâ”€ Normal: [2,3,3,4,4] = balanced, around mean
â”‚  â”‚  â””â”€ Attacker: [5,5,5,1,1] = bimodal, polarized
â”‚  â”œâ”€ Temporal pattern:
â”‚  â”‚  â”œâ”€ Normal: Spread over weeks/months
â”‚  â”‚  â””â”€ Attacker: Burst in single day
â”‚  â””â”€ Item selection:
â”‚     â”œâ”€ Normal: Items they've actually seen/used
â”‚     â””â”€ Attacker: Random or strategically targeted
â”œâ”€ Detection Formula:
â”‚  â”œâ”€ variance(user) = Î£(r - mean)Â² / n
â”‚  â””â”€ Example:
â”‚     â”œâ”€ Normal: [2,3,3,4,4]
â”‚     â”‚  â”œâ”€ Mean = 3.2
â”‚     â”‚  â””â”€ Variance = (0.04+0.04+0.04+0.64+0.64)/5 = 0.28
â”‚     â”œâ”€ Attacker: [5,5,5,1,1]
â”‚     â”‚  â”œâ”€ Mean = 3.2
â”‚     â”‚  â””â”€ Variance = (3.24+3.24+3.24+4.84+4.84)/5 = 3.88
â”‚     â””â”€ Ratio: 3.88/0.28 = 13.9Ã— higher! âš ï¸
â””â”€ âš ï¸  CRITICAL: High variance = likely attacker!

ğŸ¯ ATTACK TYPES (Impact %)
â”œâ”€ Random Attack (0-5% impact) - Weakest
â”‚  â”œâ”€ Rate random items with random ratings
â”‚  â””â”€ No pattern, easily detected
â”œâ”€ Average Attack (5-15% impact)
â”‚  â”œâ”€ Rate target item: 5 stars
â”‚  â”œâ”€ Rate popular items: 3 stars (average)
â”‚  â””â”€ Slight variance, moderate impact
â”œâ”€ Bandwagon Attack (15-30% impact)
â”‚  â”œâ”€ Target item: 5 stars
â”‚  â”œâ”€ Popular items: 5 stars
â”‚  â”œâ”€ Unpopular items: 1 star
â”‚  â””â”€ Higher impact, moderate detection difficulty
â”œâ”€ Love-Hate Attack (20-40% impact) - Strongest
â”‚  â”œâ”€ Target item: 5 stars (maximize target)
â”‚  â”œâ”€ Competitor items: 1 star (minimize competition)
â”‚  â”œâ”€ Others: Strategic (1, 3, or 5 based on impact)
â”‚  â””â”€ Most dangerous, hardest to detect
â””â”€ Sybil Attack - Distributed
   â”œâ”€ Multiple coordinated fake accounts
   â”œâ”€ Can execute complex strategies
   â””â”€ Hardest to detect (network-level attack)

ğŸ›¡ï¸ DEFENSE STRATEGIES
â”œâ”€ 1. Trust-Weighted Collaborative Filtering
â”‚  â”œâ”€ Use explicit trust instead of similarity
â”‚  â”œâ”€ Attackers have NO trust (no history)
â”‚  â”œâ”€ Formula: rÌ‚ = Î£ trust(u,v) Ã— r_vj / Î£ trust
â”‚  â””â”€ Effectiveness: Very high (attackers isolated)
â”œâ”€ 2. Robust Matrix Factorization
â”‚  â”œâ”€ Use L1 norm instead of L2 norm
â”‚  â”‚  â”œâ”€ L1 penalty: Î»|w| (linear)
â”‚  â”‚  â””â”€ L2 penalty: Î»wÂ² (quadratic, current)
â”‚  â”œâ”€ L1 makes outliers less influential
â”‚  â””â”€ Attacks affect fewer items
â”œâ”€ 3. Outlier Detection & Removal
â”‚  â”œâ”€ Identify suspicious accounts via variance
â”‚  â”œâ”€ Remove before training RS
â”‚  â””â”€ Risk: False positives (legitimate users flagged)
â””â”€ 4. Ensemble Methods
   â”œâ”€ Multiple algorithms = multiple defense layers
   â”œâ”€ Attackers fool one, not all
   â”œâ”€ Final prediction = aggregate (average, median)
   â””â”€ More robust but slower

ğŸ° MULTI-ARMED BANDITS (Exploration-Exploitation)
â”œâ”€ Problem: Balance between trying new items vs recommending known good
â”œâ”€ Îµ-Greedy Algorithm:
â”‚  â”œâ”€ With probability Îµ: Explore random arm (Îµ=0.1 typical)
â”‚  â”œâ”€ With probability 1-Îµ: Exploit best arm so far (0.9)
â”‚  â”œâ”€ Simple and fast
â”‚  â”œâ”€ Regret: O(T) linear - suboptimal
â”‚  â””â”€ Used in: Early-stage recommendations
â””â”€ UCB (Upper Confidence Bound) - Better!
   â”œâ”€ Select arm maximizing: Î¼Ì‚_a + âˆš(ln(t) / n_a)
   â”œâ”€ Auto-balances: Uncertainty + empirical mean
   â”‚  â”œâ”€ New arm (high uncertainty) = higher UCB
   â”‚  â””â”€ Tested arm (low uncertainty) = lower UCB
   â”œâ”€ No need for Îµ parameter (automatic)
   â”œâ”€ Regret: O(log T) optimal!
   â””â”€ Used in: Contextual bandits, online learning

ğŸ“Š LEARNING TO RANK (LTR)
â”œâ”€ Problem: How to train model for ranking quality?
â”œâ”€ Pointwise Approach:
â”‚  â”œâ”€ Input: (query, document, relevance score)
â”‚  â”œâ”€ Loss: MSE or cross-entropy (regression)
â”‚  â”œâ”€ Treats each doc independently
â”‚  â”œâ”€ âŒ Ignores relative ranking
â”‚  â””â”€ Use: Baseline, simple systems
â”œâ”€ Pairwise Approach: â­ MOST COMMON
â”‚  â”œâ”€ Input: (query, doc A > doc B)
â”‚  â”‚  â””â”€ Pair where A is more relevant than B
â”‚  â”œâ”€ Loss: Hinge loss (margin between pairs)
â”‚  â”œâ”€ Learns relative ordering
â”‚  â”œâ”€ âœ… Considers ranking structure
â”‚  â””â”€ Use: LambdaRank, RankNet
â””â”€ Listwise Approach:
   â”œâ”€ Input: (query, full ranking list)
   â”œâ”€ Loss: NDCG (or other ranking metric)
   â”œâ”€ Optimizes full ranking quality
   â”œâ”€ âœ… Directly optimizes final metric
   â””â”€ Use: LambdaMART, ListNet (when NDCG is critical)

ğŸ‘¥ GROUP RECOMMENDER SYSTEMS
â”œâ”€ Problem: Recommend to group of users, not single person
â”œâ”€ Aggregation Strategies:
â”‚  â”œâ”€ Average: r_G = Î£r_u / |G|
â”‚  â”‚  â”œâ”€ Fair (treats all equally)
â”‚  â”‚  â”œâ”€ Example: [5,3,4]/3 = 4.0
â”‚  â”‚  â””â”€ âŒ May satisfy nobody (4 stars to all)
â”‚  â”œâ”€ Least Misery: r_G = min(r_u)
â”‚  â”‚  â”œâ”€ Conservative (nobody dislikes)
â”‚  â”‚  â”œâ”€ Example: min(5,3,4) = 3
â”‚  â”‚  â””â”€ âŒ Often too low (limited choices)
â”‚  â”œâ”€ Most Pleasure: r_G = max(r_u)
â”‚  â”‚  â”œâ”€ Optimistic (maximize happiness)
â”‚  â”‚  â”œâ”€ Example: max(5,3,4) = 5
â”‚  â”‚  â””â”€ âŒ Ignores minority dislike
â”‚  â””â”€ Median: r_G = median(r_u)
â”‚     â”œâ”€ Balanced compromise
â”‚     â”œâ”€ Example: median(5,3,4) = 4
â”‚     â””â”€ âœ… Often best balance
â””â”€ Variants: Threshold aggregation, weighted voting

ğŸ“Š MULTI-CRITERIA RECOMMENDATION
â”œâ”€ Problem: Single rating inadequate (multiple dimensions matter)
â”œâ”€ Multiple dimensions:
â”‚  â”œâ”€ Users Ã— Items Ã— Criteria
â”‚  â”œâ”€ Example movie: [Plot, Music, Effects, Acting]
â”‚  â””â”€ Each rated separately
â”œâ”€ Tensor Approach:
â”‚  â”œâ”€ 3-way tensor: n_users Ã— n_items Ã— n_criteria
â”‚  â”œâ”€ Factorize: U Ã— I Ã— C
â”‚  â””â”€ Predict each criterion separately
â”œâ”€ Weighted Aggregation:
â”‚  â”œâ”€ Example movie ratings:
â”‚  â”‚  â”œâ”€ Plot: 5 stars
â”‚  â”‚  â”œâ”€ Music: 3 stars
â”‚  â”‚  â”œâ”€ Effects: 4 stars
â”‚  â”‚  â””â”€ Acting: 4 stars
â”‚  â”œâ”€ Weights (user preferences): [0.4, 0.2, 0.2, 0.2]
â”‚  â”œâ”€ Overall: 0.4Ã—5 + 0.2Ã—3 + 0.2Ã—4 + 0.2Ã—4 = 4.2
â”‚  â””â”€ Dynamic weights: Can change per user/context
â””â”€ Benefits: Better satisfaction, domain-specific evaluation
    </div>
    """, unsafe_allow_html=True)

# Footer
st.divider()
st.markdown("""
    <div style="text-align: center; color: #888888; margin-top: 20px;">
        <p><strong>ARD 401 - Recommender Systems Mind Maps</strong></p>
        <p>All Units | Vertical Format | 100% Legible</p>
    </div>
""", unsafe_allow_html=True)
